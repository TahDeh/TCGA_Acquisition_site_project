{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle \n",
    "import os\n",
    "import openslide\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Lambda\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.backend import bias_add, constant\n",
    "\n",
    "\n",
    "\n",
    "#Batch Preprocessing \n",
    "\n",
    "\n",
    "def preprocessing_fn(input_batch, network_input_patch_width = 1000):\n",
    "    org_input_size = tf.shape(input_batch)[1]\n",
    "\n",
    "    scaled_input_batch = tf.cast(input_batch, 'float') / 255.\n",
    "\n",
    "    resized_input_batch = tf.cond(tf.equal(org_input_size, network_input_patch_width),\n",
    "                                  lambda: scaled_input_batch,\n",
    "                                  lambda: tf.image.resize(scaled_input_batch,\n",
    "                                                          (network_input_patch_width, network_input_patch_width)))\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    data_format = \"channels_last\"\n",
    "    mean_tensor = constant(-np.array(mean))\n",
    "\n",
    "    standardized_input_batch = bias_add(resized_input_batch, mean_tensor, data_format)\n",
    "    standardized_input_batch /= std\n",
    "\n",
    "    return standardized_input_batch\n",
    "\n",
    "\n",
    "\n",
    "#load DenseNet with KimiaNet's weights\n",
    "#available at:\n",
    "#https://kimialab.uwaterloo.ca/kimia/index.php/sdm_downloads/kimianet-weights/\n",
    "\n",
    "weights_address = '\\\\kimianet weights\\\\KimiaNetKerasWeights_final.h5'\n",
    "dnx = DenseNet121(include_top=False, weights=weights_address, input_shape=(1000, 1000, 3), pooling='avg')\n",
    "kn_feature_extractor = Model(inputs=dnx.input, outputs=GlobalAveragePooling2D()(dnx.layers[-3].output))\n",
    "\n",
    "kn_feature_extractor_seq = Sequential([Lambda(preprocessing_fn,\n",
    "                                                  arguments={'network_input_patch_width': 1000},\n",
    "                                                  input_shape=(None, None, 3), dtype=tf.uint8)])\n",
    "\n",
    "kn_feature_extractor_seq.add(kn_feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a function that returns a patch by coordinates obtained from 5x magnification and returns the patch at 20x mag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_file = \"slide_file.svs\"\n",
    "slide = openslide.open_slide(slide_file)\n",
    "\n",
    "\n",
    "def get_a_patch(slide, x, y, step1=1, step2=1):\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    mag = int(slide.properties['openslide.objective-power'])\n",
    "\n",
    "    if (mag == 20):\n",
    "        x = x * 8\n",
    "        y = y * 8\n",
    "        \n",
    "        step1 = int(step1 * 1000)\n",
    "        step2 = int(step2 * 1000)\n",
    "        \n",
    "        img = slide.read_region((x, y), level=0, size=(step1, step2)).convert('RGB')\n",
    "\n",
    "    else:\n",
    "        x = x * 16\n",
    "        y = y * 16\n",
    "        \n",
    "        step1 = int(step1 * 2000)\n",
    "        step2 = int(step2 * 2000)\n",
    "\n",
    "        img = slide.read_region((x, y), level=0, size=(step1, step2)).convert('RGB').resize((int(step1/2), int(step2/2)))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract deep features from tissue patches of size 1000x1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "import pickle\n",
    "\n",
    "#read patches and save them first\n",
    "#here we load saved patches and get deep features\n",
    "image_path = \"\\\\image_path\"\n",
    "os.chdir(image_path)\n",
    "image_files = os.listdir()\n",
    "images = np.array([skimage.io.imread(x) for x in image_files])\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in range(len(image_files)):\n",
    "\n",
    "    img = images[i]\n",
    "    f = kn_feature_extractor_seq.predict(tf.expand_dims(img, axis = 0))\n",
    "    dfs.append(f)\n",
    "    \n",
    "    \n",
    "with open(\"dfs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(conv_dfs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
